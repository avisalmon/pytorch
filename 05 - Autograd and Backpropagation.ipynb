{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb7f42b2-db9e-4f81-b630-ec62233b6a26",
   "metadata": {},
   "source": [
    "### PyTorch Autograd and Backpropagation\n",
    "\n",
    "Understanding PyTorch’s autograd and backpropagation is crucial for efficiently designing and training models in machine learning. This chapter introduces these essential concepts with straightforward explanations and examples.\n",
    "\n",
    "#### Introduction to Autograd\n",
    "\n",
    "PyTorch provides an automatic differentiation system called **autograd**. It tracks all operations on `torch.Tensor` objects with the `requires_grad` attribute set to `True`. By doing so, it creates a **computation graph**, which enables efficient computation of gradients—needed for optimizing models via backpropagation. This makes PyTorch a preferred choice for tasks involving deep learning.\n",
    "\n",
    "#### Computational Graphs\n",
    "\n",
    "In PyTorch, each tensor operation creates a node in a computational graph. Tensors with `requires_grad=True` will be traced, and the entire history of computation will be captured in the graph. This graph is dynamic and automatically adjusted as tensors become unused (e.g., they go out of scope), which can be more efficient than static graph systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2a5257-fd1a-436c-9a6b-a86e1cff0a35",
   "metadata": {},
   "source": [
    "### Example: Simple Autograd Usage\n",
    "\n",
    "Let’s see a basic example of using autograd in PyTorch:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7159ad4-d331-4eb7-85fa-a8e4ee86d212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of x (dz/dx): 12.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a tensor and set requires_grad=True to track its operations\n",
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "# Perform some operations\n",
    "y = x**2  # y = x^2\n",
    "z = 2 * y + 3  # z = 2 * x^2 + 3\n",
    "\n",
    "# Perform backpropagation to compute gradients\n",
    "z.backward()  # Compute dz/dx\n",
    "\n",
    "# Print the gradient of x\n",
    "print(f\"Gradient of x (dz/dx): {x.grad}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fc2fc2c-5a58-481b-b149-b3fa28be114d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x=tensor([-2.3873,  1.7180,  0.2499], requires_grad=True)\n",
      "z=tensor(-0.2796, grad_fn=<MeanBackward0>)\n",
      "\n",
      "grad dz/dx = tensor([0.6667, 0.6667, 0.6667])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Create tensors\n",
    "x = torch.randn(3, requires_grad = True )\n",
    "print(f'{x=}')\n",
    "y = x * 2\n",
    "z = y.mean ()\n",
    "print(f'{z=}')\n",
    "# Backpropagation\n",
    "z.backward ()\n",
    "\n",
    "# Gradients\n",
    "print(f'\\ngrad dz/dx = {x.grad}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d741243-955b-4187-89cc-a237ecdfa199",
   "metadata": {},
   "source": [
    "### Explanation of the Example\n",
    "\n",
    "In this example, we:\n",
    "- Created a tensor `x` with `requires_grad=True`.\n",
    "- Performed operations on `x` to build a computational graph.\n",
    "- Used `backward()` on `z` to compute the gradient of `z` with respect to `x`.\n",
    "- Accessed `x.grad` to view the computed gradients.\n",
    "\n",
    "### Backpropagation Explained\n",
    "\n",
    "**Backpropagation** is an algorithm used to compute the gradient of a loss function with respect to all weights in a neural network. It works by applying the chain rule of differentiation across the layers in the network, allowing for efficient gradient computation needed for optimization algorithms like gradient descent.\n",
    "\n",
    "#### How Backpropagation Works:\n",
    "1. **Forward Pass**: During the forward pass, the inputs propagate through the network, and outputs are computed based on the current weights of the network.\n",
    "2. **Loss Calculation**: The difference between the predicted output and the actual target is used to compute the loss.\n",
    "3. **Backward Pass**: During backpropagation, the gradient of the loss function with respect to each parameter (weight) is computed. The gradients are calculated using the chain rule by propagating gradients backward from the output layer to the input layer.\n",
    "\n",
    "This allows for efficient updating of weights during training, using optimization techniques like stochastic gradient descent (SGD).\n",
    "\n",
    "#### PyTorch and Backpropagation:\n",
    "\n",
    "In PyTorch, when you call `.backward()` on the final output tensor, PyTorch automatically:\n",
    "- Traverses the computational graph in reverse (from output back to input).\n",
    "- Computes the gradients for all tensors in the graph that have `requires_grad=True`.\n",
    "\n",
    "These gradients can then be used by optimization algorithms to update the weights of a neural network and minimize the loss function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b628df-798f-43ee-b999-3c2ffc586005",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
